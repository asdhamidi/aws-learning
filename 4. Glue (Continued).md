# üß© **AWS Glue ‚Äì Day 3 Notes: Writing PySpark Glue ETL Script Manually**

---

## üìò 1. Introduction

AWS Glue is a **fully managed ETL (Extract, Transform, Load) service** by AWS.  
It simplifies the process of data preparation, transformation, and cataloging for analytics, ML, and reporting.

Unlike Glue Studio‚Äôs visual jobs, here we‚Äôll write **custom PySpark scripts manually** and run them as Glue Jobs.

---

## ‚öôÔ∏è 2. Glue Architecture Overview

|Component|Description|
|---|---|
|**AWS Glue Data Catalog**|Central metadata repository for all datasets. Stores schemas, table definitions, partitions, and data locations.|
|**Crawlers**|Automatically scan data sources (S3, JDBC, DynamoDB, etc.) and populate/update the Data Catalog.|
|**ETL Jobs**|PySpark or Python scripts that transform and load data.|
|**Triggers**|Define when Glue Jobs or Crawlers should run. (Manual, Scheduled, Event-based)|
|**Workflows**|Chain multiple jobs and triggers together to create a pipeline.|
|**Connections**|Store credentials and configurations to access external data sources like RDS, Redshift, etc.|

---

## üïµÔ∏è‚Äç‚ôÇÔ∏è 3. Crawlers

Crawlers **discover and register metadata** into the Data Catalog, allowing Glue (and other AWS services like Athena or Redshift Spectrum) to query datasets directly.

### üîπ 3.1 Functions of Crawlers

1. **Scan data stores** (S3, RDS, DynamoDB, etc.)
    
2. **Infer schema and data types**
    
3. **Create or update** tables in the **Glue Data Catalog**
    
4. Identify **partitions** automatically (e.g., `year=2024/month=10`)
    

---

### üîπ 3.2 Types of Crawlers

|Type|Source|Example|
|---|---|---|
|**S3 Crawler**|Scans CSV, JSON, Parquet, ORC, Avro files in S3|Logs or event data|
|**JDBC Crawler**|Scans databases via JDBC|RDS, MySQL, PostgreSQL|
|**DynamoDB Crawler**|Scans DynamoDB tables|NoSQL ingestion|
|**Custom Crawler**|Custom formats or connections|Avro from on-prem storage|

---

### üîπ 3.3 CLI Commands for Crawlers

```bash
# Create a crawler
aws glue create-crawler \
    --name sales-csv-crawler \
    --role AWSGlueServiceRole \
    --database-name retail_db \
    --targets '{"S3Targets": [{"Path": "s3://company-data/raw/sales/"}]}'

# Start the crawler
aws glue start-crawler --name sales-csv-crawler

# Get crawler status
aws glue get-crawler --name sales-csv-crawler
```

---

## ‚è∞ 4. Triggers

Triggers define **when and how Glue Jobs or Crawlers execute**.

---

### üîπ 4.1 Types of Triggers

|Type|Description|Example|
|---|---|---|
|**On-Demand**|Manually started by user or API|Ad-hoc ETL job runs|
|**Scheduled**|Runs on a predefined CRON schedule|Daily refresh at 3 AM|
|**Event-Based**|Runs automatically after another job/crawler finishes|Workflow chaining|

---

### üîπ 4.2 CLI Commands for Triggers

```bash
# Scheduled trigger
aws glue create-trigger \
  --name daily-trigger \
  --type SCHEDULED \
  --schedule "cron(0 3 * * ? *)" \
  --actions JobName=my-etl-job

# On-demand trigger
aws glue create-trigger \
  --name manual-trigger \
  --type ON_DEMAND \
  --actions JobName=my-etl-job

# Event-based trigger
aws glue create-trigger \
  --name post-crawl-trigger \
  --type CONDITIONAL \
  --predicate '{"Conditions": [{"LogicalOperator": "EQUALS", "JobName": "my-crawler", "State": "SUCCEEDED"}]}' \
  --actions JobName=my-etl-job
```

---

## üß† 5. AWS Glue Job Fundamentals

### üîπ 5.1 Glue Job Types

|Job Type|Language|Use Case|
|---|---|---|
|**Glue ETL (Spark)**|PySpark / Scala|Distributed ETL jobs|
|**Python Shell Job**|Python|Lightweight data prep or orchestration|
|**Ray Jobs**|Python|Parallel processing for ML/analytics workloads|

---

### üîπ 5.2 Core Components

|Component|Description|
|---|---|
|**Script Location**|PySpark code stored in S3 (`s3://my-bucket/scripts/etl_job.py`)|
|**IAM Role**|Grants permissions to read/write S3, access Glue, and write logs|
|**Worker Type**|`Standard`, `G.1X`, `G.2X`, `G.025X` depending on memory and vCPU needs|
|**Glue Version**|Determines Spark version (e.g., Glue 4.0 ‚Üí Spark 3.3, Python 3.10)|
|**Job Parameters**|Runtime arguments like `--input_path`, `--output_path`, etc.|

---

## üíª 6. PySpark ETL Scripts

Below are **multiple PySpark code patterns** for Glue ETL Jobs.

---

### üß© Example 1: CSV ‚Üí Parquet (Basic ETL)

```python
import sys
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql.functions import col

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path', 'output_path'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Step 1: Read CSV
df = spark.read.option("header", True).csv(args['input_path'])

# Step 2: Clean and cast types
df_clean = (
    df.dropna(subset=["id", "date"])
      .withColumn("price", col("price").cast("double"))
      .withColumn("date", col("date").cast("date"))
)

# Step 3: Write as partitioned Parquet
df_clean.write \
    .mode("overwrite") \
    .partitionBy("date") \
    .parquet(args['output_path'], compression="snappy")

job.commit()
```

üß© **Run via CLI**

```bash
aws glue start-job-run \
  --job-name csv-to-parquet \
  --arguments='--input_path=s3://company-data/raw/ --output_path=s3://company-data/processed/'
```

---

### üß© Example 2: JSON ‚Üí Parquet (Filtering + GZIP Compression)

```python
df = spark.read.json("s3://company-data/raw/json/")
df.printSchema()

df_filtered = df.filter(col("status") == "active")

df_filtered.write \
    .mode("overwrite") \
    .parquet("s3://company-data/cleaned/json/", compression="gzip")
```

---

### üß© Example 3: Using Glue Data Catalog (DynamicFrame)

```python
dynamic_frame = glueContext.create_dynamic_frame.from_catalog(
    database="retail_db",
    table_name="transactions_raw"
)

df = dynamic_frame.toDF()
df_filtered = df.filter(col("amount") > 0)

df_filtered.write.parquet(
    "s3://company-data/transactions/cleaned/",
    compression="snappy"
)
```

---

### üß© Example 4: Schema Mapping and Type Conversion (DynamicFrame)

```python
from awsglue.transforms import *
from awsglue.dynamicframe import DynamicFrame

source = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": ["s3://company-data/raw/"]},
    format="csv",
    format_options={"withHeader": True}
)

mapped = ApplyMapping.apply(
    frame=source,
    mappings=[
        ("id", "string", "id", "string"),
        ("amount", "string", "amount", "double"),
        ("date", "string", "date", "date")
    ]
)

glueContext.write_dynamic_frame.from_options(
    frame=mapped,
    connection_type="s3",
    connection_options={"path": "s3://company-data/cleaned/"},
    format="parquet",
    transformation_ctx="datasink"
)
```

---

### üß© Example 5: Incremental Load with Bookmarks

```python
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="sales_db",
    table_name="orders_raw",
    transformation_ctx="datasource",
    additional_options={"jobBookmarkKeys": ["order_id"], "jobBookmarkKeysSortOrder": "asc"}
)

# Transform and write
glueContext.write_dynamic_frame.from_options(
    frame=datasource,
    connection_type="s3",
    connection_options={"path": "s3://sales-data/incremental/"},
    format="parquet",
    transformation_ctx="datasink"
)

job.commit()  # important for bookmark state
```

---

## ‚òÅÔ∏è 7. Deploying and Running Glue Job

### üîπ 7.1 Upload Script to S3

```bash
aws s3 cp ./scripts/etl_script.py s3://my-glue-scripts/
```

---

### üîπ 7.2 Create Glue Job

```bash
aws glue create-job \
  --name my-csv-etl-job \
  --role AWSGlueServiceRole \
  --command '{"Name": "glueetl", "ScriptLocation": "s3://my-glue-scripts/etl_script.py"}' \
  --default-arguments '{"--job-language":"python"}' \
  --glue-version "4.0" \
  --number-of-workers 2 \
  --worker-type G.1X
```

---

### üîπ 7.3 Run Job

```bash
aws glue start-job-run --job-name my-csv-etl-job
```

---

### üîπ 7.4 Monitor Job

- Go to **AWS Console ‚Üí CloudWatch Logs**
    
- Look for:
    
    - `Job failed due to Python exception`
        
    - `Schema mismatch`
        
    - `OutOfMemoryError`
        
- You can rerun failed jobs by reusing the same script.
    

---

## üßÆ 8. Optimization & Best Practices

|Area|Best Practice|
|---|---|
|**File Format**|Prefer **Parquet** or **ORC** over CSV for analytics|
|**Compression**|Use **Snappy** for fast reads, **GZIP** for smaller size|
|**Partitioning**|Always partition large datasets (e.g., by `date` or `region`)|
|**Schema Management**|Use **Glue Catalog** and **bookmarks** for incremental tracking|
|**Transformations**|For heavy transformations, consider PySpark over DynamicFrame|
|**Memory Tuning**|Adjust worker type (`G.2X`) for large jobs|
|**Logging**|Use `job.commit()` to finalize job and persist state|
