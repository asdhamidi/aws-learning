# **Amazon Redshift**

## **1. Redshift Serverless Overview**

Amazon Redshift is AWS’s **cloud data warehouse** designed for **analytical (OLAP)** workloads. It’s built on **columnar storage** and **massively parallel processing (MPP)** — meaning it stores data column-wise and distributes both data and queries across multiple nodes for high performance.

### **Core Idea**

Traditional Redshift required you to manage clusters (node count, types, resizing, etc.).  
**Redshift Serverless** removes this operational burden — AWS manages compute and storage automatically.

---

### **Key Concepts**

|Concept|Description|
|---|---|
|**Namespace**|A logical container for databases, schemas, users, and permissions — analogous to a Snowflake account or database environment.|
|**Workgroup**|The compute layer (virtual cluster) associated with a namespace. It provides the RPUs (Redshift Processing Units) for queries.|
|**RPU (Redshift Processing Unit)**|Unit of compute capacity that bundles CPU, memory, and I/O. You pay per RPU-hour.|
|**Serverless**|You don’t manage nodes. AWS automatically provisions and scales compute when queries run, then pauses when idle.|
|**Storage**|Automatically scales in S3-backed managed storage. Redshift Serverless separates compute from storage, similar to Snowflake’s architecture.|
|**Query Editor v2**|Web-based IDE to run queries, visualize results, and manage schemas directly from the AWS Console.|

---

### **How It Works (Conceptually)**

1. You create a **Namespace** — this holds metadata (users, databases, schemas).
    
2. You create a **Workgroup** — this defines compute isolation, scaling behavior, and networking.
    
3. When you execute a query, AWS spins up compute automatically.
    
4. You’re billed only for the compute (RPU-seconds) during active use and for the GBs of data stored monthly.
    

---

### **CLI Example**

```bash
aws redshift-serverless list-workgroups --region ap-south-1
aws redshift-serverless list-namespaces --region ap-south-1
```

These commands verify your setup by listing existing workgroups and namespaces.

---

## **2. Create Redshift Serverless Environment**

Creating a Redshift Serverless setup involves two entities: the **Namespace** (metadata and storage) and **Workgroup** (compute).

---

### **Step 1: Create Namespace**

This step defines the **logical data environment** — think of it as creating a “database cluster” without physical hardware.

```bash
aws redshift-serverless create-namespace \
  --namespace-name de-namespace \
  --admin-username adminuser \
  --admin-user-password 'StrongPass123!' \
  --region ap-south-1
```

- The namespace holds all your databases, schemas, and user credentials.
    
- `adminuser` is the root-level user who can create additional users and roles.
    

---

### **Step 2: Create Workgroup**

The Workgroup provides **compute capacity**. You can specify a base capacity in RPUs (like 8, 16, 32).

```bash
aws redshift-serverless create-workgroup \
  --workgroup-name de-workgroup \
  --base-capacity 32 \
  --namespace-name de-namespace \
  --region ap-south-1
```

- **Base capacity** defines the minimum compute available. Redshift automatically scales up/down from this baseline.
    
- Each Workgroup can be associated with one Namespace, but you can have multiple workgroups for different workloads (ETL vs BI).
    

---

### **Step 3: Connect via Query Editor v2**

Once created, connect to Redshift using Query Editor v2:

```sql
SELECT current_database(), current_user;
SELECT 1;
```

If these return results successfully, your environment is ready.

---

## **3. IAM Role for Redshift to Access S3**

Redshift doesn’t have direct S3 access by default. You must **grant permissions via an IAM role**.

This is crucial because Redshift uses this IAM role when running `COPY`, `UNLOAD`, and Spectrum queries.

---

### **Trust Policy**

This trust relationship allows Redshift to assume the IAM role:

```json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": {"Service": "redshift.amazonaws.com"},
    "Action": "sts:AssumeRole"
  }]
}
```

It means: “Redshift service is trusted to assume this role.”

---

### **Create Role and Attach Policy**

```bash
aws iam create-role --role-name RedshiftRole \
  --assume-role-policy-document file://trust-policy.json

aws iam attach-role-policy \
  --role-name RedshiftRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
```

- The attached managed policy allows read access to S3 objects.
    
- For write access (e.g., `UNLOAD`), you can add `AmazonS3FullAccess`.
    

---

### **Attach Role to Workgroup**

```bash
aws redshift-serverless update-workgroup \
  --workgroup-name de-workgroup \
  --add-iam-roles arn:aws:iam::<acct>:role/RedshiftRole
```

This binds the IAM role to the compute environment, so Redshift can use it transparently for S3 operations.

---

## **4. Loading Data from S3 (COPY Command)**

The **COPY command** is the backbone of Redshift ingestion — optimized for parallel, bulk loads from S3.

---

### **Conceptual Flow**

1. Files on S3 (CSV, JSON, Parquet) →
    
2. Redshift fetches them via IAM role →
    
3. Data gets distributed across slices (parallel nodes) →
    
4. Stored column-wise for compression and query performance.
    

---

### **Table DDL Example**

```sql
CREATE SCHEMA IF NOT EXISTS curated;
CREATE TABLE curated.flights (
  id INT,
  fl_date DATE,
  op_carrier VARCHAR(10),
  dep_delay INT,
  arr_delay INT
);
```

---

### **COPY Examples**

**Parquet (recommended):**

```sql
COPY curated.flights
FROM 's3://developers-test-bucket1/curated/flights/'
IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftRole'
FORMAT AS PARQUET;
```

- Parquet loads are faster and schema-aware.
    
- Redshift automatically maps Parquet column types.
    

**CSV Example:**

```sql
COPY curated.flights
FROM 's3://developers-test-bucket1/curated/flights/'
IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftRole'
CSV GZIP IGNOREHEADER 1;
```

---

### **Validation and Troubleshooting**

```sql
SELECT COUNT(*), MIN(fl_date), MAX(fl_date) FROM curated.flights;
SELECT * FROM stl_load_errors LIMIT 10;
```

`stl_load_errors` gives detailed row-level issues for bad records (invalid type, missing column, etc.).

---

### **Performance Tips**

- Split files into **multiple 100–1000 MB chunks** for parallel loading.
    
- Match Redshift column types to file schema.
    
- Use **manifest files** to ensure all files are loaded exactly once.
    
- Prefer **Parquet + GZIP** for optimal compression and scan speed.
    

---

## **5. Exporting Data to S3 (UNLOAD Command)**

**UNLOAD** is the inverse of COPY — it writes Redshift query results back to S3.

This is useful for:

- Sharing data with downstream services (Athena, Glue, SageMaker).
    
- Storing aggregates for dashboards.
    
- Offloading cold data from Redshift to cheaper S3.
    

---

### **Example**

```sql
UNLOAD ('SELECT fl_date, AVG(dep_delay) AS avg_delay
         FROM curated.flights
         GROUP BY fl_date')
TO 's3://developers-test-bucket1/marts/daily_delay_'
IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftRole'
PARQUET;
```

---

### **Athena Round-trip**

Once the data is unloaded:

1. Point a **Glue Crawler** to `s3://developers-test-bucket1/marts/`.
    
2. Let it infer the schema and store it in the Glue Catalog.
    
3. Query directly via Athena:
    

```sql
SELECT * FROM marts.daily_delay LIMIT 5;
```

This allows **cross-service analytics** without duplication.

---

## **6. Spectrum (External Schema & Glue Catalog Integration)**

Redshift Spectrum enables **federated queries** — Redshift can query **data directly in S3** without loading it.

---

### **How It Works**

- Spectrum acts as a **query federation layer** between Redshift and the Glue Catalog.
    
- Glue stores metadata (schema, partition info) for S3 data.
    
- Redshift pushes down filters/aggregates to Spectrum, minimizing data transfer.
    

---

### **Create External Schema**

```sql
CREATE EXTERNAL SCHEMA spectrum
FROM DATA CATALOG
DATABASE 'data_lake_db'
IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftSpectrumRole'
CREATE EXTERNAL DATABASE IF NOT EXISTS;
```

Now, `spectrum` becomes a virtual schema in Redshift that maps to external S3 data.

---

### **Example Query**

```sql
SELECT COUNT(*) FROM spectrum.sales;

SELECT a.id, b.region
FROM curated.flights a
JOIN spectrum.sales b ON a.id = b.id;
```

This merges warehouse and lake data — Redshift handles metadata joins efficiently by using **predicate pushdown**.

---

### **Required IAM Policies**

- `AmazonS3ReadOnlyAccess`
    
- `AWSGlueConsoleFullAccess`
    
- Fine-grained: `glue:Get*`, `glue:List*`
    

---

## **7. Table Design & Performance Tuning**

Redshift performance heavily depends on **distribution** and **sort** strategies.

---

### **Distribution Keys**

Determine **how data is split across nodes**.

- `DISTKEY`: Choose the join key used most frequently.  
    Ensures co-located joins → avoids network shuffling.
    
- `AUTO`: Redshift optimizes small tables automatically.
    

### **Sort Keys**

Define **how data is ordered** on disk.  
Improves filtering and range queries (e.g., on date columns).

```sql
CREATE TABLE curated.sales (
  id INT DISTKEY,
  sale_date DATE SORTKEY,
  amount DECIMAL(10,2)
);
```

---

### **Analyze and Vacuum**

```sql
ANALYZE curated.sales;  -- Refresh stats for optimizer
VACUUM curated.sales;   -- Reclaim space and reorder
```

While Serverless automates some of this, manual maintenance ensures consistent performance.

---

## **8. Incremental Loads & Upserts**

Redshift doesn’t natively support `MERGE` in all regions, but you can simulate it.

### **Typical Pattern**

1. Load incremental data into a **staging table**.
    
2. Use **DELETE + INSERT** or `MERGE` (if enabled).
    

```sql
COPY staging.sales
FROM 's3://bucket/new_data/'
IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftRole'
FORMAT AS PARQUET;

DELETE FROM curated.sales
USING staging.sales
WHERE curated.sales.id = staging.sales.id;

INSERT INTO curated.sales
SELECT * FROM staging.sales;
```

This pattern ensures data freshness with minimal downtime.

---

## **9. Hybrid Pipeline Integration Patterns**

|Integration|Use Case|
|---|---|
|**S3 → Redshift (COPY)**|Ingest curated or raw data from your lake into the warehouse.|
|**Redshift → S3 (UNLOAD)**|Export marts or analytical results back to the lake for reuse.|
|**Spectrum Joins**|Query lake data alongside warehouse tables.|
|**Airflow / Lambda**|Automate COPY and UNLOAD jobs programmatically.|
|**Step Functions**|Orchestrate full ETL pipelines from extraction to reporting.|

This forms a **lakehouse architecture** similar to Snowflake’s external stages and data sharing.

---

## **10. Security and Cost Optimization**

- **Networking:** Restrict access using **VPC security groups** and **private endpoints**.
    
- **IAM Roles:** Create separate roles for COPY/UNLOAD vs Spectrum.
    
- **Compute Limits:** Use **RPU limits** and pause idle workgroups to control cost.
    
- **Housekeeping:** Delete unused namespaces/workgroups to stop storage billing.
    

```bash
aws redshift-serverless delete-workgroup --workgroup-name de-workgroup
aws redshift-serverless delete-namespace --namespace-name de-namespace
```

---

## **11. CLI Quick Reference**

```bash
# List Serverless Workgroups & Namespaces
aws redshift-serverless list-workgroups
aws redshift-serverless list-namespaces

# Describe Environment
aws redshift-serverless get-workgroup --workgroup-name de-workgroup
aws redshift-serverless get-namespace --namespace-name de-namespace

# Pause / Resume Workgroup
aws redshift-serverless pause-workgroup --workgroup-name de-workgroup
aws redshift-serverless resume-workgroup --workgroup-name de-workgroup

# List SQL Queries
aws redshift-data list-statements --region ap-south-1

# Execute SQL via CLI
aws redshift-data execute-statement \
  --workgroup-name de-workgroup \
  --database dev \
  --sql "SELECT COUNT(*) FROM curated.flights;" \
  --region ap-south-1
```
## **12. Executing Queries via CLI**

Amazon Redshift Serverless provides the **`redshift-data`** CLI commands to execute SQL statements programmatically. This is equivalent to running queries in Query Editor v2 but works directly from your terminal, scripts, or automation pipelines.

---

### **Prerequisites**

1. AWS CLI configured with permissions for `redshift-data`.
    
2. Workgroup must exist and be active.
    
3. Database and user must exist inside your namespace.
    

---

### **Run a Simple Query**

```bash
aws redshift-data execute-statement \
  --workgroup-name de-workgroup \
  --database dev \
  --sql "SELECT COUNT(*) FROM curated.flights;" \
  --region ap-south-1
```

**Explanation:**

- `--workgroup-name`: The compute environment to run the query on.
    
- `--database`: Database inside the namespace.
    
- `--sql`: The SQL query string.
    
- The command returns a **statement ID**, not the result immediately.
    

---

### **Fetch Query Results**

After running the query, use the statement ID to retrieve results:

```bash
aws redshift-data get-statement-result \
  --id <statement-id> \
  --region ap-south-1
```

- The results are returned in **JSON format**.
    
- For large queries, you may need to paginate through results using `NextToken`.
    

---

### **List and Track Queries**

You can track all queries executed via CLI or programmatically:

```bash
# List last 10 statements
aws redshift-data list-statements --region ap-south-1 --max-items 10

# Describe a specific statement
aws redshift-data describe-statement --id <statement-id> --region ap-south-1
```

- Useful for automation pipelines to check query status (`STARTED`, `FINISHED`, `FAILED`).
    

---

### **Advanced Usage**

- **Run queries asynchronously** in scripts.
    
- Integrate with **Airflow, Lambda, or Step Functions** to orchestrate SQL jobs.
    
- Combine with **UNLOAD/COPY** commands in automation.
    

```bash
aws redshift-data execute-statement \
  --workgroup-name de-workgroup \
  --database dev \
  --sql "UNLOAD ('SELECT * FROM curated.flights') TO 's3://bucket/marts/' IAM_ROLE 'arn:aws:iam::<acct>:role/RedshiftRole' PARQUET;" \
  --region ap-south-1
```

