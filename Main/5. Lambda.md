# âš¡ **AWS Lambda

---

## ğŸ“˜ 1. Introduction

**AWS Lambda** is a **serverless compute service** that lets you run code **without managing servers or infrastructure**.  
It executes your code **in response to events** (e.g., file upload to S3, message in Kinesis, or API call), automatically scales, and charges you only for the time your code runs.

In a data engineering context, Lambda acts as a **lightweight, event-driven ETL or orchestration layer**, especially between storage, streaming, and processing components.

---

## âš™ï¸ 2. Core Concepts

|Component|Description|
|---|---|
|**Function**|The deployed code package (Python, Node.js, etc.) executed by Lambda runtime.|
|**Trigger / Event Source**|Automatically invokes your function when an event occurs (S3 upload, Kinesis message, Step Function call).|
|**IAM Role**|Grants permissions to access AWS services (S3, Glue, DynamoDB, CloudWatch, etc.).|
|**Environment Variables**|Store configuration details such as bucket names, Glue job names, or thresholds.|
|**Execution Context**|Provides temporary compute environment including CPU, RAM, `/tmp` space, and cached dependencies.|

---

## âš™ï¸ 3. How Lambda Works

Lambda follows an **event-driven** model:

1. **Event Source** (e.g., S3, Kinesis, EventBridge) detects a new event.
    
2. It **invokes your Lambda function** asynchronously or synchronously.
    
3. Lambda **executes your code** within an ephemeral container.
    
4. **Logs and metrics** are automatically sent to CloudWatch.
    
5. The environment is **reused** for subsequent invocations (warm start).
    

---

## âš™ï¸ 4. Lambda Execution Model

|Characteristic|Description|
|---|---|
|**Scaling**|Lambda scales automatically by running multiple concurrent instances of your function.|
|**Billing**|Charged per millisecond of execution and memory allocated.|
|**Limits**|Default timeout 15 mins, `/tmp` storage 512 MB (can be extended with EFS).|
|**Concurrency**|Can be reserved or left dynamic.|
|**Retries**|Automatic retries for asynchronous invocations.|

---

## ğŸ§© 5. Typical Data Engineering Use Cases

|Use Case|Description|
|---|---|
|**S3-triggered transformations**|Detect file upload, validate, and start Glue ETL job or small Python processing.|
|**Streaming preprocessing**|Consume messages from **Kinesis**, clean/aggregate data, and push to S3 or DynamoDB.|
|**ETL orchestration**|Serve as glue code between multiple AWS services (S3 â†’ Glue â†’ Redshift).|
|**API-based transformations**|Trigger ETL jobs or lookups via Lambda integrated with API Gateway.|
|**Automation / Notifications**|Send Slack, SNS, or email alerts on data quality issues.|

---

## ğŸš€ 6. Hands-on Fundamentals

### ğŸ”¹ Example 1: Basic Lambda

```python
def lambda_handler(event, context):
    print("Event received:", event)
    return {"statusCode": 200, "body": "Hello from Lambda!"}
```

âœ… _Invoke manually_ from AWS Console â†’ Test tab.  
âœ… _Logs_ visible in CloudWatch under `/aws/lambda/<function-name>`.

---

### ğŸ”¹ Example 2: S3 â†’ Lambda Trigger

```python
import boto3

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        print(f"New file uploaded to {bucket}: {key}")
```

When a file is uploaded to the S3 bucket, this Lambda automatically runs.  
You can then chain actions (validation, compression, Glue trigger, etc.).

---

## âš—ï¸ 7. Lambda + Glue Integration

**Goal:** Start a Glue ETL job automatically when new raw data lands in S3.

### ğŸ”¹ Workflow

```
S3 â†’ Lambda (triggered) â†’ Glue Job (PySpark ETL) â†’ Processed S3 zone
```

### ğŸ”¹ Example

```python
import boto3, json

glue = boto3.client('glue')

def lambda_handler(event, context):
    response = glue.start_job_run(JobName='sales-etl-job')
    print("Glue job started:", response['JobRunId'])
    return {"statusCode": 200, "body": json.dumps("Glue job triggered")}
```

**IAM Policy Additions:**

- `glue:StartJobRun`
    
- `s3:GetObject`
    
- `logs:*`
    

âœ… Logs and job status appear in **CloudWatch** and **Glue Console**.

---

## ğŸŒŠ 8. Lambda + Kinesis Stream Processing

Kinesis is a **real-time streaming data service**, similar to **Apache Kafka**.  
Lambda can be configured as a **Kinesis consumer**.

### ğŸ”¹ Workflow

```
Producers (apps, devices) â†’ Kinesis Stream â†’ Lambda â†’ S3/DB
```

### ğŸ”¹ Example

```python
import base64, json

def lambda_handler(event, context):
    for record in event['Records']:
        data = json.loads(base64.b64decode(record['kinesis']['data']))
        print("Received:", data)
```

**Typical uses:**

- Real-time analytics pipelines
    
- Anomaly detection or alerts
    
- Pre-aggregating micro-batches before storing to S3
    

---

## ğŸ§­ 9. Monitoring, Logging & Error Handling

### ğŸ”¹ CloudWatch Integration

Every Lambda function automatically publishes:

- **Logs** (`print()` output)
    
- **Metrics** (invocation count, duration, errors, throttles)
    

Accessible via:

> AWS Console â†’ Lambda â†’ Monitor tab â†’ View Logs in CloudWatch

### ğŸ”¹ Error Handling & DLQ

For **asynchronous events (S3, SNS)**, Lambda retries on failure (up to 2 more times).  
After retries, the failed event can be sent to a **DLQ (Dead Letter Queue)**.

|DLQ Type|Description|
|---|---|
|**SQS Queue**|Stores failed event payloads for later reprocessing.|
|**SNS Topic**|Publishes failed payloads as notifications (e.g., email, alerts).|

Configured under:

> Lambda â†’ Configuration â†’ Asynchronous invocation â†’ DLQ

---

## ğŸ’¡ 10. Environment Variables

You can inject configuration values like:

```python
import os

BUCKET = os.environ['TARGET_BUCKET']
JOB_NAME = os.environ['GLUE_JOB']

def lambda_handler(event, context):
    print("Writing to bucket:", BUCKET)
```

âœ… Avoids hardcoding paths or names.  
âœ… Useful across dev/test/prod environments.

---

## ğŸ§© 11. Mini Project â€“ Serverless ETL Pipeline

### ğŸ”¹ Architecture

```
Raw S3 â†’ Lambda â†’ Glue ETL â†’ Gold S3 (Parquet)
```

### ğŸ”¹ Flow

1. Upload raw CSV/JSON to S3 (`raw/` prefix).
    
2. S3 event triggers Lambda.
    
3. Lambda starts Glue ETL job.
    
4. Glue transforms data to Parquet and writes to `gold/` prefix.
    
5. CloudWatch monitors both Lambda + Glue logs.
    

### ğŸ”¹ Deliverables

- IAM policy references
    
- Lambda configuration details
    
- Glue job screenshots
    
- Execution logs in CloudWatch
    

---

## ğŸ§  12. Advanced Bonus Topics

### ğŸ”¹ A. AWS Step Functions for Multi-step Orchestration

**Purpose:** Chain multiple Lambda functions, Glue jobs, and API calls into a single **state machine**.

#### Example Workflow:

```
S3 â†’ Trigger Lambda â†’ Step Function â†’ 
    [Validate Data] â†’ [Run Glue Job] â†’ [Post ETL Notification]
```

#### How It Starts:

- A **Lambda** or **EventBridge rule** calls:
    
    ```python
    boto3.client('stepfunctions').start_execution(
        stateMachineArn='arn:aws:states:xxx:MyStateMachine',
        input=json.dumps(event)
    )
    ```
    
- This _explicitly_ starts the workflow.
    
- Step Function states can include:
    
    - `Task` (run a Lambda or Glue job)
        
    - `Choice` (conditional branching)
        
    - `Parallel` (run tasks concurrently)
        

> ğŸ”¸ **Note:** Step Functions never start automatically on Lambda triggers â€” they must be explicitly invoked.

---

### ğŸ”¹ B. Mounting EFS for Large Dependencies

If your Lambda needs heavy libraries (like `pandas`, `numpy`, or `sklearn`) that exceed the 250 MB deployment limit:

1. Create an **Amazon EFS** file system.
    
2. Attach it to the Lambdaâ€™s VPC and mount path (e.g., `/mnt/efs`).
    
3. Preload dependencies into EFS:
    
    ```bash
    pip install pandas -t /mnt/efs/python/
    ```
    
4. Access in your Lambda:
    
    ```python
    import sys
    sys.path.append("/mnt/efs/python/")
    import pandas as pd
    ```
    

> EFS gives persistent, large storage across invocations and supports larger dependency packages.

---

### ğŸ”¹ C. Lambda Layers for Reusable Dependencies

Lambda **Layers** are versioned, shareable bundles of dependencies or utility code.

#### Structure

```
lambda_layer/
â””â”€â”€ python/
    â”œâ”€â”€ utils/
    â”‚   â”œâ”€â”€ logging.py
    â”‚   â””â”€â”€ helpers.py
    â””â”€â”€ requirements.txt
```

#### Creation

```bash
zip -r layer.zip python/
aws lambda publish-layer-version --layer-name my-utils --zip-file fileb://layer.zip
```

#### Usage in Lambda

Attach the layer and import normally:

```python
from utils.logging import log_event
```

âœ… Reduces deployment size  
âœ… Promotes code reuse  
âœ… Ideal for shared connectors, loggers, or validation helpers

---

## âš™ï¸ 13. Best Practices for Lambda in Data Engineering

|Area|Best Practice|
|---|---|
|**Memory & Timeout**|Start with 512 MB and 60 s, increase gradually based on load.|
|**Cold Starts**|Use Provisioned Concurrency for critical pipelines.|
|**Retries & DLQ**|Always configure retry logic and DLQs for event-driven Lambdas.|
|**Security**|Principle of least privilege in IAM roles.|
|**Modularity**|Keep functions focused â€” one Lambda per task.|
|**Monitoring**|Use CloudWatch + X-Ray for tracing and latency insights.|
|**Data Validation**|Validate S3 file format and schema before triggering Glue jobs.|

---

## ğŸ§© 14. Summary

AWS Lambda is the **core serverless glue** in modern data engineering stacks.  
It seamlessly connects services like **S3**, **Kinesis**, **Glue**, and **Step Functions** to build **cost-effective, auto-scaling, event-driven ETL pipelines**.

|Concept|Description|
|---|---|
|**Trigger-based**|Automatically responds to new data or stream events.|
|**Integration-rich**|Works with S3, Kinesis, Glue, SNS, SQS, and Step Functions.|
|**Lightweight orchestration**|Excellent for workflow coordination and micro-transformations.|
|**Scalable and economical**|Pay-per-invocation, no infrastructure overhead.|
