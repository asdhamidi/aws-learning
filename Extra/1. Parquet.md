# **Apache Parquet**

## **1. Overview**

Apache Parquet is a **columnar storage format** for structured data. It’s designed for **analytical workloads**, where **reading a few columns from a large dataset** is common.

Unlike row-based formats (CSV, JSON, Avro), Parquet stores data **column-wise**, enabling:

- Better **compression** (similar values are stored together).
    
- Faster **query performance** for column projections.
    
- Efficient **encoding and predicate pushdown**.
    

---

### **Key Features**

|Feature|Description|
|---|---|
|**Columnar Storage**|Stores data by columns instead of rows, improving scan speed for analytical queries.|
|**Schema-aware**|Stores metadata about columns and types, enabling type safety and faster reads.|
|**Compression**|Each column can use an appropriate compression codec (Snappy, GZIP, Brotli, ZSTD).|
|**Splittable**|Large files can be read in parallel by multiple workers or nodes in distributed systems.|
|**Cross-platform**|Parquet files can be read/written by Java, Python (PyArrow, Pandas), Spark, Hive, Presto, and more.|
|**Efficient I/O**|Only columns required by the query are read, reducing disk and network usage.|
|**Predicate Pushdown**|Column statistics allow skipping irrelevant row groups (e.g., “date >= 2025-01-01”) without scanning all data.|

---

### **Comparison with Other Formats**

|Format|Row vs Column|Compression|Use Case|
|---|---|---|---|
|CSV|Row|Low (text-based)|Simple data exchange, small datasets|
|JSON|Row|Low|Semi-structured data, logs|
|Avro|Row|Medium|Write-heavy pipelines, schema evolution|
|Parquet|Column|High|Analytics, big data, OLAP queries|
|ORC|Column|High|Hadoop/Spark workloads, similar to Parquet|

**Why Columnar Matters:**  
For queries like `SELECT date, sales FROM big_table WHERE region='APAC'`, only the **date** and **sales** columns are read, not the entire row — drastically reducing I/O and improving query speed.

---

## **2. Internal Structure of Parquet**

Parquet files are **self-describing** and structured into layers for performance:

### **2.1 Row Groups**

- A **row group** is a horizontal partition of the table (e.g., 50k rows per group).
    
- Each row group contains data for **all columns**, but stored separately.
    

### **2.2 Column Chunks**

- Inside a row group, each column is stored as a **column chunk**.
    
- This allows reading a subset of columns efficiently.
    

### **2.3 Pages**

- Each column chunk is further divided into **pages** (data pages, dictionary pages, index pages).
    
- Pages are compressed and encoded individually.
    

### **2.4 Metadata**

- File footer stores:
    
    - Column names, types
        
    - Compression codec
        
    - Encodings
        
    - Number of rows, row groups
        
- Metadata enables **schema evolution**, predicate pushdown, and parallel reads.
    

---

### **2.5 Compression & Encoding**

- Compression reduces storage and I/O.
    
- Common codecs:
    
    - **Snappy**: fast, moderate compression (default for Hadoop ecosystem)
        
    - **GZIP**: slower, higher compression
        
    - **Brotli/ZSTD**: higher compression ratio, CPU-intensive
        
- Encoding schemes:
    
    - **Dictionary Encoding**: for low-cardinality columns
        
    - **Run-Length Encoding**: for repeating values
        
    - **Delta Encoding**: for sequences or sorted numbers
        

---

## **3. Advantages of Parquet**

1. **High-performance analytics**  
    Columnar storage + predicate pushdown = faster queries on large datasets.
    
2. **Reduced storage footprint**  
    Column-wise compression is more effective than row-wise.
    
3. **Interoperable**  
    Works across big data frameworks: Spark, Hive, Presto, Flink, Pandas, Dask.
    
4. **Schema evolution**  
    Add columns without breaking existing readers.
    
5. **Splittable files**  
    Enables distributed processing for parallelism.
    
6. **Efficient network usage**  
    Only the required columns travel over the network.
    

---

## **4. Typical Use Cases**

|Use Case|Why Parquet|
|---|---|
|Data Lake Storage|Columnar + compressed → cheap storage + fast reads|
|ETL/ELT Pipelines|Incremental updates → write once, read many|
|Analytics|Only necessary columns scanned → fast BI queries|
|Machine Learning|Efficient feature extraction from large datasets|
|Interoperability|Acts as a standard format for multiple frameworks|

---

## **5. Reading and Writing Parquet (Generic)**

### **Python (Pandas / PyArrow)**

```python
import pandas as pd

# Writing a DataFrame to Parquet
df.to_parquet("data.parquet", engine='pyarrow', compression='snappy')

# Reading Parquet into DataFrame
df = pd.read_parquet("data.parquet", engine='pyarrow')
```

### **Spark**

```python
# Write to Parquet
df.write.parquet("s3://bucket/data/")

# Read from Parquet
df = spark.read.parquet("s3://bucket/data/")
```

**Key Point:** Most big data frameworks **default to Parquet for OLAP workloads** due to its speed and compression.

---

## **6. Best Practices**

1. **File size:** Keep Parquet files **50–500 MB per file** for optimal parallel processing.
    
2. **Row group size:** ~50k–100k rows per group to balance memory usage and I/O.
    
3. **Column ordering:** Place frequently queried columns first.
    
4. **Compression codec:** Use **Snappy** for general purpose, **GZIP/ZSTD** if storage is more important than CPU.
    
5. **Partitioning:** Organize files by high-cardinality columns (e.g., date, region) to enable **partition pruning** during queries.
    
6. **Schema evolution:** Add new columns to the end; avoid changing types.
    

---

## **7. AWS & Parquet**

AWS services integrate tightly with Parquet, especially in **analytics and data lake architectures**.

### **7.1 S3 Data Lake**

- Store Parquet files in S3 for cheap, scalable, and schema-aware storage.
    
- Combine with **Glue Catalog** for metadata management.
    

### **7.2 Redshift Spectrum**

- Query S3 Parquet files **in place** via external tables without loading into Redshift.
    
- Supports predicate pushdown and column pruning for fast analytics.
    

### **7.3 Athena**

- Athena can directly query Parquet files stored in S3.
    
- Partitioning + Parquet = lightning-fast queries without moving data.
    

### **7.4 EMR / Spark**

- Parquet is the default format for Spark/EMR analytical pipelines.
    
- Efficient for ETL jobs — read → transform → write back in Parquet.
    

### **7.5 AWS Best Practices**

1. Use **Snappy compression** for query speed.
    
2. Partition by common query filters (date, region, customer_id).
    
3. Use Glue Crawlers to register Parquet schemas in the Data Catalog.
    
4. Combine with **Athena, Redshift Spectrum, and Lake Formation** for analytics pipelines.
    

---

Perfect — let’s go deep and visualize what a Parquet dataset looks like **on disk or in cloud storage**, breaking down the folders, files, naming conventions, and what each piece means. I’ll keep it **generic first**, then tie it into **AWS S3 patterns**.

---

# **8. Parquet Storage Layout

When you save a dataset in Parquet (especially large ones), it’s rarely just **one file**. Parquet datasets are often organized into **folders, partitions, and multiple Parquet files**, depending on framework (Spark, Pandas, Hive) and partitioning strategy.
## **8.1 Generic Layout**

Assume you have a dataset of **flights** with columns: `date`, `carrier`, `origin`, `dest`, `dep_delay`.

### **Without Partitioning**

```
flights/
├── part-00000-abc123.parquet
├── part-00001-def456.parquet
├── part-00002-ghi789.parquet
└── _metadata
```

**Explanation:**

- `part-00000-abc123.parquet`, etc.
    
    - Each file is a **chunk of the dataset**, often 50–500 MB for parallel processing.
        
    - Spark/Hive or other frameworks split data into multiple files for **distributed reading/writing**.
        
    - The **hash or UUID** in the filename ensures uniqueness across tasks/workers.
        
- `_metadata`
    
    - Optional. Contains **dataset-wide metadata**, like schema, column types, and row group info.
        
    - Used by Spark, Hive, Presto, or Athena to read schema without scanning all files.
        
- `_common_metadata` (sometimes present)
    
    - Consolidates metadata across multiple files.
        
    - Helps tools read large datasets efficiently without opening every file.
        

---

### **With Partitioning**

Partitioning organizes data by **column values** for faster queries and pruning. Suppose we partition by `year` and `month`:

```
flights/
├── year=2024/
│   ├── month=01/
│   │   ├── part-00000.parquet
│   │   └── part-00001.parquet
│   └── month=02/
│       ├── part-00000.parquet
│       └── part-00001.parquet
└── year=2025/
    ├── month=01/
    │   ├── part-00000.parquet
    │   └── part-00001.parquet
    └── month=02/
        ├── part-00000.parquet
        └── part-00001.parquet
```

**Explanation:**

- **Partition folders**: `year=2024/`, `month=01/`
    
    - Represent **filterable values**. Query engines (Athena, Spark, Redshift Spectrum) can skip irrelevant partitions entirely → **query pruning**.
        
- **Files inside partitions**: contain **row groups** and column chunks for the specific partition.
    
    - E.g., `flights/year=2025/month=01/part-00001.parquet` has only January 2025 flights.
        
- **Number of files per partition** depends on **parallelism during write**. Spark may produce multiple files per partition; Pandas typically writes a single file.
    

---

### **8.2 File Naming & Conventions**

- `part-00000-<uuid>.parquet`
    
    - `part-00000`: sequential number from the writer task.
        
    - `<uuid>`: unique hash to prevent collision in distributed writes.
        
- `_metadata` / `_common_metadata`
    
    - Schema information. Enables schema evolution without reading every file.
        
- Partition folders:
    
    - `column=value/` is a **standard for query engines** (Hive style).
        
    - Supports **automatic predicate pushdown**.
        
- Row groups within Parquet files:
    
    - Typically 50k–100k rows.
        
    - Each row group has **column chunks** and compressed **pages**.
        
    - Row groups are the unit of parallel read within a single file.
        

---

### **8.3 Visualizing the Layout Conceptually**

```
Dataset folder
├─ Partition folder(s) (optional)
│  ├─ Parquet file(s)
│  │  ├─ Row group 1
│  │  │  ├─ Column A chunk (pages)
│  │  │  ├─ Column B chunk (pages)
│  │  │  └─ Column C chunk (pages)
│  │  └─ Row group 2 ...
│  └─ Parquet file 2 ...
└─ _metadata / _common_metadata
```

**Takeaways:**

- **Folders = partitions** → high-level pruning.
    
- **Files = row group container** → unit of parallel processing.
    
- **Column chunks = storage for a single column** → enables columnar efficiency.
    
- **Pages = smallest read unit** → compressed, encoded data.
    

---

## **8.4 AWS S3 & Parquet Storage Patterns**

On AWS, Parquet is usually stored in **S3 buckets** with **partitioned folder hierarchy**, often combined with a **Glue Catalog**.

Example:

```
s3://my-data-lake/flights/
├── year=2024/
│   ├── month=01/
│   │   ├── part-00000.parquet
│   │   └── part-00001.parquet
│   └── month=02/
│       └── part-00000.parquet
└── year=2025/
    └── month=01/
        ├── part-00000.parquet
        └── part-00001.parquet
```

**AWS-specific notes:**

1. **Glue Catalog / Athena**
    
    - Crawlers detect schema and partitions automatically.
        
    - Query engines can **skip irrelevant partitions** → high performance.
        
2. **Redshift Spectrum**
    
    - Treats S3 Parquet files as **external tables**.
        
    - Reads only required columns and partitions.
        
3. **File size recommendations**
    
    - 100–500 MB per file → optimal for Redshift, Athena, or Spark parallel reads.
        
    - Too small → too many files, overhead increases.
        
    - Too large → harder to read in parallel.
        
4. **Partitioning best practices**
    
    - Use **high-cardinality, frequently filtered columns** (date, region, customer_id).
        
    - Avoid excessive partitioning → small files per partition degrade performance.
        
5. **Compression & Encoding**
    
    - Snappy is common for AWS analytics pipelines (good speed/compression balance).
        
    - GZIP/ZSTD for storage efficiency at some CPU cost.
        

---

### **8.5 Key Conceptual Takeaways**

- **Partition folders** → logical pruning.
    
- **Parquet files** → distributed chunks for parallelism.
    
- **Row groups** → in-file parallel reads.
    
- **Column chunks/pages** → compression and selective column read.
    
- **_metadata files** → schema definition for engines without scanning data.
    

> Think of a Parquet dataset as a **tree**: folders partition by column values, files store row groups, and each row group stores column chunks in pages.
